#!/usr/bin/env python3
"""
å¤„ç†æ‰€æœ‰H5æ–‡ä»¶çš„é‡å»ºè„šæœ¬
åŸºäºç°æœ‰æˆåŠŸçš„EVREALæ•°æ®ç»“æ„ï¼Œä¸ºæ‰€æœ‰H5æ–‡ä»¶ï¼ˆåŒ…æ‹¬åŸå§‹æ–‡ä»¶ã€Unetã€Unetsimpleç­‰ï¼‰åˆ›å»ºé‡å»ºç»“æœ

æ ¸å¿ƒåŸç†ï¼š
1. å¤åˆ¶ç°æœ‰æˆåŠŸçš„EVREALæ•°æ®ç»“æ„ä½œä¸ºæ¨¡æ¿
2. åªæ›¿æ¢äº‹ä»¶æ•°æ®æ–‡ä»¶ï¼ˆevents_ts.npy, events_xy.npy, events_p.npyï¼‰
3. é‡æ–°ç”ŸæˆåŒ¹é…çš„image_event_indices.npyæ–‡ä»¶
4. è¿è¡ŒEVREALé‡å»ºå¹¶å¤åˆ¶åˆ°æŒ‡å®šç›®å½•

ä½¿ç”¨æ–¹æ³•:
source ~/miniconda3/etc/profile.d/conda.sh && conda activate Umain2 && python process_additional_h5_files.py <dataset_name>

ç¤ºä¾‹:
python process_additional_h5_files.py lego2  # å¤„ç†lego2ç›®å½•ä¸‹çš„æ‰€æœ‰H5æ–‡ä»¶

Author: Claude Code Assistant  
Date: 2025-09-25
"""

import os
import sys
import h5py
import shutil
import numpy as np
from pathlib import Path
from typing import List, Dict
import json
import time

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('.')
sys.path.append('..')

try:
    from modules.evreal_integration import EVREALIntegrationConfig, EVREALIntegration
except ImportError as e:
    print(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿åœ¨æ­£ç¡®çš„é¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œæ­¤è„šæœ¬")
    sys.exit(1)


class AllH5Processor:
    """æ‰€æœ‰H5æ–‡ä»¶å¤„ç†å™¨"""
    
    def __init__(self, dataset_name: str):
        self.dataset_name = dataset_name
        self.dataset_dir = Path("datasets") / dataset_name
        self.h5_dir = self.dataset_dir / "events_h5"
        self.base_evreal_dir = self.dataset_dir / "events_evreal"
        
        # éªŒè¯å¿…è¦ç›®å½•å­˜åœ¨
        self._validate_directories()
    
    def _validate_directories(self):
        """éªŒè¯å¿…è¦ç›®å½•å­˜åœ¨"""
        if not self.dataset_dir.exists():
            raise FileNotFoundError(f"æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {self.dataset_dir}")
        if not self.h5_dir.exists():
            raise FileNotFoundError(f"H5ç›®å½•ä¸å­˜åœ¨: {self.h5_dir}")
        if not self.base_evreal_dir.exists():
            raise FileNotFoundError(f"åŸºç¡€EVREALç›®å½•ä¸å­˜åœ¨: {self.base_evreal_dir}")
        
        base_sequence_dir = self.base_evreal_dir / "sequence"
        if not base_sequence_dir.exists():
            raise FileNotFoundError(f"åŸºç¡€EVREAL sequenceç›®å½•ä¸å­˜åœ¨: {base_sequence_dir}")
    
    def get_all_h5_files(self) -> List[Path]:
        """è·å–éœ€è¦å¤„ç†çš„æ‰€æœ‰H5æ–‡ä»¶"""
        h5_files = []
        
        for h5_file in self.h5_dir.glob("*.h5"):
            # åªè·³è¿‡å¤‡ä»½æ–‡ä»¶
            if "backup" in h5_file.name or "wrong" in h5_file.name:
                print(f"è·³è¿‡å¤‡ä»½æ–‡ä»¶: {h5_file.name}")
                continue
                
            h5_files.append(h5_file)
        
        h5_files.sort()
        print(f"æ‰¾åˆ° {len(h5_files)} ä¸ªH5æ–‡ä»¶å¾…å¤„ç†:")
        for h5_file in h5_files:
            print(f"  - {h5_file.name}")
        return h5_files
    
    def extract_suffix_from_filename(self, h5_file: Path) -> str:
        """ä»H5æ–‡ä»¶åæå–åç¼€"""
        stem = h5_file.stem
        
        # å°è¯•ç§»é™¤åŸºç¡€æ¨¡å¼å‰ç¼€
        base_patterns = [
            f"{self.dataset_name}_sequence_new",
            f"{self.dataset_name}_sequence", 
            f"{self.dataset_name}_train_events",
            f"{self.dataset_name}"
        ]
        
        for base_pattern in base_patterns:
            if stem.startswith(base_pattern):
                suffix = stem.replace(base_pattern, "")
                if suffix.startswith("_"):
                    return suffix[1:]  # ç§»é™¤å‰å¯¼ä¸‹åˆ’çº¿
                elif suffix == "":
                    return "original"
        
        # å…¶ä»–æƒ…å†µä½¿ç”¨å®Œæ•´æ–‡ä»¶åä½œä¸ºåç¼€
        return stem.replace(".", "_")
    
    def create_temp_evreal_structure(self, suffix: str) -> Path:
        """åˆ›å»ºä¸´æ—¶EVREALæ•°æ®ç»“æ„"""
        temp_evreal_dir = self.dataset_dir / f"events_evreal_temp_{suffix}"
        
        print(f"åˆ›å»ºä¸´æ—¶EVREALç»“æ„: {temp_evreal_dir}")
        
        # æ¸…ç†å¯èƒ½å­˜åœ¨çš„æ—§ä¸´æ—¶ç›®å½•
        if temp_evreal_dir.exists():
            shutil.rmtree(temp_evreal_dir)
        
        # å¤åˆ¶åŸºç¡€EVREALç»“æ„
        shutil.copytree(self.base_evreal_dir, temp_evreal_dir)
        
        return temp_evreal_dir
    
    def replace_event_data(self, h5_file: Path, temp_evreal_dir: Path) -> Dict:
        """æ›¿æ¢äº‹ä»¶æ•°æ®"""
        print(f"æ›¿æ¢äº‹ä»¶æ•°æ®: {h5_file.name}")
        
        sequence_dir = temp_evreal_dir / "sequence"
        
        # è¯»å–H5æ–‡ä»¶äº‹ä»¶æ•°æ®
        with h5py.File(h5_file, 'r') as f:
            if 'events' in f and isinstance(f['events'], h5py.Group):
                # åˆ†ç»„æ ¼å¼: events/t, events/x, events/y, events/p
                events_t = f['events/t'][:]
                events_x = f['events/x'][:]
                events_y = f['events/y'][:]
                events_p = f['events/p'][:]
                print(f"  H5äº‹ä»¶æ•°é‡: {len(events_t)} (åˆ†ç»„æ ¼å¼)")
            elif 'events' in f and isinstance(f['events'], h5py.Dataset):
                # æ•°ç»„æ ¼å¼: events[:, [t,x,y,p]]
                events_data = f['events'][:]
                events_t = events_data[:, 0]
                events_x = events_data[:, 1] 
                events_y = events_data[:, 2]
                events_p = events_data[:, 3]
                print(f"  H5äº‹ä»¶æ•°é‡: {len(events_data)} (æ•°ç»„æ ¼å¼)")
            else:
                raise ValueError(f"æœªè¯†åˆ«çš„H5æ–‡ä»¶æ ¼å¼: {h5_file}")
        
        # è½¬æ¢ä¸ºEVREALæ ¼å¼
        events_ts = events_t.astype(np.float64) / 1000000.0  # å¾®ç§’è½¬ç§’
        events_xy = np.column_stack([events_x.astype(np.int16), events_y.astype(np.int16)])
        events_p = events_p.astype(np.int8)
        
        # ä¿å­˜æ–°çš„äº‹ä»¶æ•°æ®
        np.save(sequence_dir / "events_ts.npy", events_ts)
        np.save(sequence_dir / "events_xy.npy", events_xy)
        np.save(sequence_dir / "events_p.npy", events_p)
        
        print(f"  âœ“ æ—¶é—´èŒƒå›´: {events_ts.min():.6f}s - {events_ts.max():.6f}s")
        print(f"  âœ“ ç©ºé—´èŒƒå›´: X[{events_xy[:, 0].min()}, {events_xy[:, 0].max()}], Y[{events_xy[:, 1].min()}, {events_xy[:, 1].max()}]")
        print(f"  âœ“ ææ€§åˆ†å¸ƒ: +{(events_p == 1).sum()}, -{(events_p == 0).sum()}")
        
        return {
            "num_events": len(events_ts),
            "time_range": [float(events_ts.min()), float(events_ts.max())],
            "spatial_range": {
                "x_range": [int(events_xy[:, 0].min()), int(events_xy[:, 0].max())],
                "y_range": [int(events_xy[:, 1].min()), int(events_xy[:, 1].max())]
            }
        }
    
    def regenerate_image_event_indices(self, temp_evreal_dir: Path, new_event_count: int):
        """é‡æ–°ç”ŸæˆåŒ¹é…çš„image_event_indicesæ–‡ä»¶"""
        print("é‡æ–°ç”Ÿæˆimage_event_indices.npy...")
        
        sequence_dir = temp_evreal_dir / "sequence"
        
        # è¯»å–åŸå§‹ç´¢å¼•
        base_sequence_dir = self.base_evreal_dir / "sequence"
        original_indices = np.load(base_sequence_dir / "image_event_indices.npy")
        
        # è®¡ç®—ç¼©æ”¾å› å­
        original_max_index = original_indices.max()
        scale_factor = (new_event_count - 1) / original_max_index  # -1ç¡®ä¿ä¸è¶Šç•Œ
        
        # ç”Ÿæˆæ–°çš„ç´¢å¼•
        new_indices = (original_indices * scale_factor).astype(np.int32)
        new_indices = np.clip(new_indices, 0, new_event_count - 1)
        
        # ä¿å­˜æ–°ç´¢å¼•
        np.save(sequence_dir / "image_event_indices.npy", new_indices)
        
        print(f"  âœ“ åŸå§‹ç´¢å¼•èŒƒå›´: {original_indices.min()} - {original_indices.max()}")
        print(f"  âœ“ æ–°ç´¢å¼•èŒƒå›´: {new_indices.min()} - {new_indices.max()}")
        print(f"  âœ“ ç¼©æ”¾å› å­: {scale_factor:.4f}")
    
    def run_evreal_reconstruction(self, temp_evreal_dir: Path, suffix: str) -> Dict:
        """è¿è¡ŒEVREALé‡å»º"""
        print(f"\n=== è¿è¡ŒEVREALé‡å»º: {suffix} ===")
        
        try:
            # é…ç½®EVREALé›†æˆ
            config = EVREALIntegrationConfig()
            config.dataset_name = f"{self.dataset_name}_{suffix}_fixed"
            config.dataset_dir = self.dataset_dir
            config.evreal_data_dir = temp_evreal_dir
            config.reconstruction_dir = self.dataset_dir / f"reconstruction_{suffix}"
            
            # è¿è¡Œé‡å»º
            integration = EVREALIntegration(config)
            result = integration.run_full_pipeline()
            
            if result.get("successful_methods"):
                print(f"âœ… {suffix}é‡å»ºæˆåŠŸ: {result['successful_methods']}")
                return {
                    "success": True,
                    "methods": result["successful_methods"],
                    "reconstruction_dir": config.reconstruction_dir
                }
            else:
                print(f"âŒ {suffix}é‡å»ºå¤±è´¥: æ²¡æœ‰æˆåŠŸçš„æ–¹æ³•")
                return {"success": False, "error": "æ²¡æœ‰æˆåŠŸçš„æ–¹æ³•"}
                
        except Exception as e:
            print(f"âŒ {suffix}é‡å»ºå¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    def cleanup_temp_directory(self, temp_evreal_dir: Path):
        """æ¸…ç†ä¸´æ—¶ç›®å½•"""
        try:
            if temp_evreal_dir.exists():
                shutil.rmtree(temp_evreal_dir)
                print(f"âœ“ æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_evreal_dir}")
        except Exception as e:
            print(f"âš  æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")
    
    def process_single_h5_file(self, h5_file: Path) -> Dict:
        """å¤„ç†å•ä¸ªH5æ–‡ä»¶"""
        suffix = self.extract_suffix_from_filename(h5_file)
        
        print(f"\n{'='*60}")
        print(f"å¤„ç†H5æ–‡ä»¶: {h5_file.name} -> {suffix}")
        print(f"{'='*60}")
        
        temp_evreal_dir = None
        
        try:
            # 1. åˆ›å»ºä¸´æ—¶EVREALç»“æ„
            temp_evreal_dir = self.create_temp_evreal_structure(suffix)
            
            # 2. æ›¿æ¢äº‹ä»¶æ•°æ®
            event_info = self.replace_event_data(h5_file, temp_evreal_dir)
            
            # 3. é‡æ–°ç”Ÿæˆç´¢å¼•
            self.regenerate_image_event_indices(temp_evreal_dir, event_info["num_events"])
            
            # 4. è¿è¡ŒEVREALé‡å»º
            result = self.run_evreal_reconstruction(temp_evreal_dir, suffix)
            
            result.update({
                "h5_file": h5_file.name,
                "suffix": suffix,
                "event_info": event_info
            })
            
            return result
            
        except Exception as e:
            print(f"âŒ å¤„ç†{h5_file.name}å¤±è´¥: {e}")
            return {
                "success": False,
                "h5_file": h5_file.name,
                "suffix": suffix,
                "error": str(e)
            }
        
        finally:
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            if temp_evreal_dir:
                self.cleanup_temp_directory(temp_evreal_dir)
    
    def process_all_h5_files(self):
        """å¤„ç†æ‰€æœ‰H5æ–‡ä»¶"""
        h5_files = self.get_all_h5_files()
        
        if not h5_files:
            print("âœ… æ²¡æœ‰H5æ–‡ä»¶éœ€è¦å¤„ç†")
            return
        
        print(f"\nğŸš€ å¼€å§‹å¤„ç† {len(h5_files)} ä¸ªH5æ–‡ä»¶...")
        
        results = []
        for i, h5_file in enumerate(h5_files, 1):
            print(f"\nå¤„ç†è¿›åº¦: {i}/{len(h5_files)}")
            result = self.process_single_h5_file(h5_file)
            results.append(result)
        
        # è¾“å‡ºæœ€ç»ˆç»“æœ
        self.print_final_results(results)
    
    def print_final_results(self, results: List[Dict]):
        """æ‰“å°æœ€ç»ˆå¤„ç†ç»“æœ"""
        print(f"\n{'='*60}")
        print("ğŸ‰ å¤„ç†å®Œæˆ! ç»“æœæ±‡æ€»:")
        print(f"{'='*60}")
        
        successful = [r for r in results if r.get("success", False)]
        failed = [r for r in results if not r.get("success", False)]
        
        print(f"âœ… æˆåŠŸ: {len(successful)}/{len(results)}")
        for result in successful:
            methods_count = len(result.get("methods", []))
            reconstruction_dir = result.get("reconstruction_dir", "")
            print(f"   {result['h5_file']} â†’ {reconstruction_dir.name}/ ({methods_count}ç§æ–¹æ³•)")
        
        if failed:
            print(f"\nâŒ å¤±è´¥: {len(failed)}/{len(results)}")
            for result in failed:
                error_msg = result.get("error", "æœªçŸ¥é”™è¯¯")
                print(f"   {result['h5_file']}: {error_msg}")
        
        # ç»Ÿè®¡é‡å»ºå›¾åƒ
        if successful:
            print(f"\nğŸ“Š é‡å»ºå›¾åƒç»Ÿè®¡:")
            total_images = 0
            for result in successful:
                if "reconstruction_dir" in result and result["reconstruction_dir"]:
                    reconstruction_dir = Path(result["reconstruction_dir"])
                    if reconstruction_dir.exists():
                        png_count = len(list(reconstruction_dir.rglob("*.png")))
                        method_dirs = [d for d in reconstruction_dir.iterdir() if d.is_dir()]
                        print(f"   {result['suffix']}: {png_count} å¼ å›¾åƒ, {len(method_dirs)} ç§æ–¹æ³•")
                        total_images += png_count
            
            print(f"\nğŸ“ˆ æ€»è®¡: {total_images} å¼ é‡å»ºå›¾åƒ")


def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) < 2:
        print("âŒ é”™è¯¯: è¯·æŒ‡å®šæ•°æ®é›†åç§°")
        print("ä½¿ç”¨æ–¹æ³•: python process_additional_h5_files.py <dataset_name>")
        print("ç¤ºä¾‹: python process_additional_h5_files.py lego2")
        sys.exit(1)
    
    dataset_name = sys.argv[1]
    
    print(f"ğŸ”¥ {dataset_name} æ‰€æœ‰H5æ–‡ä»¶é‡å»ºè„šæœ¬")
    print("=" * 60)
    
    # æ£€æŸ¥condaç¯å¢ƒ
    conda_env = os.environ.get('CONDA_DEFAULT_ENV', 'None')
    print(f"å½“å‰condaç¯å¢ƒ: {conda_env}")
    if conda_env != 'Umain2':
        print("âš ï¸  è­¦å‘Š: æœªåœ¨Umain2ç¯å¢ƒä¸­è¿è¡Œ!")
        print("è¯·ä½¿ç”¨: source ~/miniconda3/etc/profile.d/conda.sh && conda activate Umain2")
    
    try:
        # åˆ›å»ºå¤„ç†å™¨å¹¶è¿è¡Œ
        processor = AllH5Processor(dataset_name)
        processor.process_all_h5_files()
        
    except Exception as e:
        print(f"âŒ è„šæœ¬æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)
    
    print(f"\nğŸ¯ {dataset_name} æ‰€æœ‰H5å¤„ç†å®Œæˆ!")


if __name__ == "__main__":
    main()