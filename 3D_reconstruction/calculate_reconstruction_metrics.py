#!/usr/bin/env python3
"""
é‡å»ºå›¾åƒè´¨é‡æŒ‡æ ‡è®¡ç®—è„šæœ¬
è®¡ç®—æ‰€æœ‰é‡å»ºç»“æœä¸test/trainçœŸå€¼çš„PSNRã€SSIMã€LPIPSæŒ‡æ ‡

åŠŸèƒ½è¯´æ˜ï¼š
- å¯¹Nä¸ªH5æ–‡ä»¶çš„é‡å»ºç»“æœåˆ†åˆ«ä¸testå’ŒtrainçœŸå€¼è®¡ç®—æŒ‡æ ‡
- æ¯ä¸ªé‡å»ºæ–¹æ³•äº§ç”Ÿ2ä¸ªæŒ‡æ ‡ç»“æœï¼ˆvs test, vs trainï¼‰
- è®¡ç®—200å¼ å›¾ç‰‡çš„å¹³å‡æŒ‡æ ‡å€¼
- è¾“å‡ºæœ€ä½³ç»“æœæ’åå’Œè¯¦ç»†æŒ‡æ ‡æ–‡ä»¶

ä½¿ç”¨æ–¹æ³•:
source ~/miniconda3/etc/profile.d/conda.sh && conda activate Umain2 && python calculate_reconstruction_metrics.py <dataset_name>

ç¤ºä¾‹:
python calculate_reconstruction_metrics.py lego2

Author: Claude Code Assistant
Date: 2025-09-25
"""

import os
import sys
import cv2
import json
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import time
from tqdm import tqdm

# å›¾åƒè´¨é‡æŒ‡æ ‡è®¡ç®—
try:
    from skimage.metrics import structural_similarity as ssim
    from skimage.metrics import peak_signal_noise_ratio as psnr
except ImportError:
    print("âŒ ç¼ºå°‘scikit-imageåº“ï¼Œè¯·å®‰è£…ï¼špip install scikit-image")
    sys.exit(1)

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    print("âŒ ç¼ºå°‘torchåº“")
    TORCH_AVAILABLE = False

try:
    import lpips
    LPIPS_AVAILABLE = True
except ImportError:
    print("âš ï¸  lpipsåº“ä¸å¯ç”¨ï¼Œå°†è·³è¿‡LPIPSè®¡ç®—")
    LPIPS_AVAILABLE = False


class ReconstructionMetricsCalculator:
    """é‡å»ºå›¾åƒè´¨é‡æŒ‡æ ‡è®¡ç®—å™¨"""
    
    def __init__(self, dataset_name: str):
        self.dataset_name = dataset_name
        self.dataset_dir = Path("datasets") / dataset_name
        
        # éªŒè¯ç›®å½•å­˜åœ¨
        self._validate_directories()
        
        # åˆå§‹åŒ–LPIPSæ¨¡å‹
        if TORCH_AVAILABLE and LPIPS_AVAILABLE:
            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
            print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
            try:
                self.lpips_model = lpips.LPIPS(net='alex').to(self.device)
                self.lpips_enabled = True
            except Exception as e:
                print(f"âš ï¸  LPIPSæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
                self.lpips_enabled = False
        else:
            self.device = 'cpu'
            self.lpips_model = None
            self.lpips_enabled = False
        
        # ç»“æœå­˜å‚¨
        self.results = []
    
    def _validate_directories(self):
        """éªŒè¯å¿…è¦ç›®å½•å­˜åœ¨"""
        if not self.dataset_dir.exists():
            raise FileNotFoundError(f"æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {self.dataset_dir}")
        
        self.train_dir = self.dataset_dir / "train"
        self.test_dir = self.dataset_dir / "test"
        
        if not self.train_dir.exists():
            raise FileNotFoundError(f"è®­ç»ƒçœŸå€¼ç›®å½•ä¸å­˜åœ¨: {self.train_dir}")
        if not self.test_dir.exists():
            raise FileNotFoundError(f"æµ‹è¯•çœŸå€¼ç›®å½•ä¸å­˜åœ¨: {self.test_dir}")
    
    def get_reconstruction_directories(self) -> List[Path]:
        """è·å–æ‰€æœ‰é‡å»ºç›®å½•"""
        reconstruction_dirs = []
        
        for item in self.dataset_dir.iterdir():
            if item.is_dir() and item.name.startswith("reconstruction"):
                # è·³è¿‡æ—§çš„reconstructionç›®å½•ï¼Œåªå¤„ç†æœ‰åç¼€çš„é‡å»ºç›®å½•
                if item.name == "reconstruction":
                    print(f"  è·³è¿‡æ—§çš„é‡å»ºç›®å½•: {item.name}")
                    continue
                reconstruction_dirs.append(item)
        
        reconstruction_dirs.sort()
        print(f"æ‰¾åˆ° {len(reconstruction_dirs)} ä¸ªé‡å»ºç›®å½•:")
        for rec_dir in reconstruction_dirs:
            print(f"  - {rec_dir.name}")
        
        return reconstruction_dirs
    
    def get_method_directories(self, reconstruction_dir: Path) -> List[Path]:
        """è·å–é‡å»ºç›®å½•ä¸‹çš„æ‰€æœ‰æ–¹æ³•ç›®å½•"""
        method_dirs = []
        
        for item in reconstruction_dir.iterdir():
            if item.is_dir() and item.name.startswith("evreal_"):
                method_dirs.append(item)
        
        method_dirs.sort()
        return method_dirs
    
    def load_images(self, image_dir: Path, expected_count: int = 200) -> List[np.ndarray]:
        """åŠ è½½å›¾åƒåºåˆ—"""
        images = []
        missing_count = 0
        
        for i in range(1, expected_count + 1):
            img_path = image_dir / f"{i:04d}.png"
            if not img_path.exists():
                missing_count += 1
                continue
                
            img = cv2.imread(str(img_path))
            if img is None:
                missing_count += 1
                continue
            
            # è½¬æ¢ä¸ºRGB
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            images.append(img)
        
        if missing_count > 0:
            print(f"  âš  ç¼ºå°‘ {missing_count} å¼ å›¾åƒ")
        print(f"  åŠ è½½äº† {len(images)}/{expected_count} å¼ å›¾åƒ")
        return images
    
    def calculate_psnr_batch(self, pred_images: List[np.ndarray], gt_images: List[np.ndarray]) -> float:
        """æ‰¹é‡è®¡ç®—PSNR"""
        if len(pred_images) != len(gt_images):
            print(f"âš  å›¾åƒæ•°é‡ä¸åŒ¹é…: {len(pred_images)} vs {len(gt_images)}")
            min_len = min(len(pred_images), len(gt_images))
            pred_images = pred_images[:min_len]
            gt_images = gt_images[:min_len]
        
        psnr_values = []
        for pred, gt in zip(pred_images, gt_images):
            if pred.shape != gt.shape:
                print(f"âš  å›¾åƒå°ºå¯¸ä¸åŒ¹é…: {pred.shape} vs {gt.shape}")
                continue
            
            try:
                psnr_val = psnr(gt, pred, data_range=255)
                if not np.isnan(psnr_val) and not np.isinf(psnr_val):
                    psnr_values.append(psnr_val)
            except Exception as e:
                print(f"âš  PSNRè®¡ç®—å¤±è´¥: {e}")
                continue
        
        return np.mean(psnr_values) if psnr_values else 0.0
    
    def calculate_ssim_batch(self, pred_images: List[np.ndarray], gt_images: List[np.ndarray]) -> float:
        """æ‰¹é‡è®¡ç®—SSIM"""
        if len(pred_images) != len(gt_images):
            min_len = min(len(pred_images), len(gt_images))
            pred_images = pred_images[:min_len]
            gt_images = gt_images[:min_len]
        
        ssim_values = []
        for pred, gt in zip(pred_images, gt_images):
            if pred.shape != gt.shape:
                continue
            
            try:
                # è½¬æ¢ä¸ºç°åº¦å›¾è®¡ç®—SSIM
                pred_gray = cv2.cvtColor(pred, cv2.COLOR_RGB2GRAY) if len(pred.shape) == 3 else pred
                gt_gray = cv2.cvtColor(gt, cv2.COLOR_RGB2GRAY) if len(gt.shape) == 3 else gt
                
                ssim_val = ssim(gt_gray, pred_gray, data_range=255)
                if not np.isnan(ssim_val) and not np.isinf(ssim_val):
                    ssim_values.append(ssim_val)
            except Exception as e:
                print(f"âš  SSIMè®¡ç®—å¤±è´¥: {e}")
                continue
        
        return np.mean(ssim_values) if ssim_values else 0.0
    
    def calculate_lpips_batch(self, pred_images: List[np.ndarray], gt_images: List[np.ndarray]) -> float:
        """æ‰¹é‡è®¡ç®—LPIPS"""
        if not self.lpips_enabled:
            return 1.0  # è¿”å›é»˜è®¤å€¼ï¼Œè¡¨ç¤ºä¸å¯ç”¨
        
        if len(pred_images) != len(gt_images):
            min_len = min(len(pred_images), len(gt_images))
            pred_images = pred_images[:min_len]
            gt_images = gt_images[:min_len]
        
        lpips_values = []
        batch_size = 10  # æ‰¹é‡å¤„ç†é¿å…å†…å­˜é—®é¢˜
        
        for i in range(0, len(pred_images), batch_size):
            batch_pred = pred_images[i:i+batch_size]
            batch_gt = gt_images[i:i+batch_size]
            
            try:
                # è½¬æ¢ä¸ºtensoræ ¼å¼ [batch, 3, H, W], èŒƒå›´[-1, 1]
                pred_tensors = []
                gt_tensors = []
                
                for pred, gt in zip(batch_pred, batch_gt):
                    if pred.shape != gt.shape:
                        continue
                    
                    # å½’ä¸€åŒ–åˆ°[-1, 1]
                    pred_norm = (pred.astype(np.float32) / 255.0) * 2.0 - 1.0
                    gt_norm = (gt.astype(np.float32) / 255.0) * 2.0 - 1.0
                    
                    # HWC -> CHW
                    pred_tensor = torch.from_numpy(pred_norm.transpose(2, 0, 1)).unsqueeze(0).to(self.device)
                    gt_tensor = torch.from_numpy(gt_norm.transpose(2, 0, 1)).unsqueeze(0).to(self.device)
                    
                    pred_tensors.append(pred_tensor)
                    gt_tensors.append(gt_tensor)
                
                if pred_tensors and gt_tensors:
                    pred_batch = torch.cat(pred_tensors, dim=0)
                    gt_batch = torch.cat(gt_tensors, dim=0)
                    
                    with torch.no_grad():
                        lpips_batch = self.lpips_model(pred_batch, gt_batch)
                        lpips_values.extend(lpips_batch.cpu().numpy().flatten())
                
            except Exception as e:
                print(f"âš  LPIPSè®¡ç®—å¤±è´¥: {e}")
                continue
        
        return np.mean(lpips_values) if lpips_values else 1.0
    
    def calculate_metrics_for_pair(self, pred_dir: Path, gt_dir: Path, pair_name: str) -> Dict:
        """è®¡ç®—ä¸€å¯¹å›¾åƒç›®å½•çš„æŒ‡æ ‡"""
        print(f"  è®¡ç®—æŒ‡æ ‡: {pair_name}")
        
        # åŠ è½½å›¾åƒ
        pred_images = self.load_images(pred_dir)
        gt_images = self.load_images(gt_dir)
        
        if not pred_images or not gt_images:
            print(f"  âŒ å›¾åƒåŠ è½½å¤±è´¥: {pair_name}")
            return {
                "pair_name": pair_name,
                "psnr": 0.0,
                "ssim": 0.0, 
                "lpips": 1.0,
                "image_count": 0,
                "error": "å›¾åƒåŠ è½½å¤±è´¥"
            }
        
        # è®¡ç®—æŒ‡æ ‡
        start_time = time.time()
        
        psnr_val = self.calculate_psnr_batch(pred_images, gt_images)
        ssim_val = self.calculate_ssim_batch(pred_images, gt_images)
        lpips_val = self.calculate_lpips_batch(pred_images, gt_images)
        
        elapsed_time = time.time() - start_time
        
        result = {
            "pair_name": pair_name,
            "psnr": float(psnr_val),
            "ssim": float(ssim_val),
            "lpips": float(lpips_val),
            "image_count": min(len(pred_images), len(gt_images)),
            "calculation_time_s": float(elapsed_time)
        }
        
        print(f"    PSNR: {psnr_val:.3f}, SSIM: {ssim_val:.3f}, LPIPS: {lpips_val:.3f} ({elapsed_time:.1f}s)")
        
        return result
    
    def process_all_reconstructions(self):
        """å¤„ç†æ‰€æœ‰é‡å»ºç»“æœ"""
        reconstruction_dirs = self.get_reconstruction_directories()
        
        if not reconstruction_dirs:
            print("âŒ æœªæ‰¾åˆ°é‡å»ºç›®å½•")
            return
        
        print(f"\nğŸš€ å¼€å§‹è®¡ç®— {len(reconstruction_dirs)} ä¸ªé‡å»ºç›®å½•çš„æŒ‡æ ‡...")
        
        # éå†æ¯ä¸ªé‡å»ºç›®å½•
        for rec_dir in tqdm(reconstruction_dirs, desc="å¤„ç†é‡å»ºç›®å½•"):
            rec_name = rec_dir.name  # å¦‚ï¼šreconstruction_original
            
            print(f"\n{'='*60}")
            print(f"å¤„ç†: {rec_name}")
            print(f"{'='*60}")
            
            # è·å–æ–¹æ³•ç›®å½•
            method_dirs = self.get_method_directories(rec_dir)
            
            if not method_dirs:
                print(f"  âš  {rec_name} ä¸­æœªæ‰¾åˆ°æ–¹æ³•ç›®å½•")
                continue
            
            print(f"  æ‰¾åˆ° {len(method_dirs)} ä¸ªæ–¹æ³•ç›®å½•")
            
            # éå†æ¯ä¸ªæ–¹æ³•
            for method_dir in method_dirs:
                method_name = method_dir.name.replace("evreal_", "")  # å¦‚ï¼še2vid
                
                print(f"\n--- æ–¹æ³•: {method_name} ---")
                
                # åˆ†åˆ«ä¸trainå’Œtestè®¡ç®—æŒ‡æ ‡
                for gt_type in ["train", "test"]:
                    gt_dir = self.dataset_dir / gt_type
                    pair_name = f"{rec_name}_{method_name}_vs_{gt_type}"
                    
                    result = self.calculate_metrics_for_pair(method_dir, gt_dir, pair_name)
                    
                    # æ·»åŠ å…ƒä¿¡æ¯
                    result.update({
                        "dataset_name": self.dataset_name,
                        "reconstruction_name": rec_name,
                        "method_name": method_name,
                        "ground_truth_type": gt_type,
                        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
                    })
                    
                    self.results.append(result)
        
        print(f"\nâœ… å®Œæˆæ‰€æœ‰æŒ‡æ ‡è®¡ç®—ï¼Œå…± {len(self.results)} ä¸ªç»“æœ")
    
    def save_results(self):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        if not self.results:
            print("âŒ æ²¡æœ‰ç»“æœå¯ä¿å­˜")
            return
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = self.dataset_dir / "metrics_results"
        output_dir.mkdir(exist_ok=True)
        
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜è¯¦ç»†ç»“æœJSON
        json_file = output_dir / f"detailed_metrics_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜CSVè¡¨æ ¼
        csv_file = output_dir / f"metrics_summary_{timestamp}.csv"
        df = pd.DataFrame(self.results)
        df.to_csv(csv_file, index=False, encoding='utf-8')
        
        print(f"âœ… è¯¦ç»†ç»“æœå·²ä¿å­˜: {json_file}")
        print(f"âœ… CSVè¡¨æ ¼å·²ä¿å­˜: {csv_file}")
        
        return json_file, csv_file
    
    def analyze_best_results(self):
        """åˆ†ææœ€ä½³ç»“æœ"""
        if not self.results:
            print("âŒ æ²¡æœ‰ç»“æœå¯åˆ†æ")
            return
        
        df = pd.DataFrame(self.results)
        
        print(f"\n{'='*60}")
        print("ğŸ† æœ€ä½³ç»“æœåˆ†æ")
        print(f"{'='*60}")
        
        # æŒ‰æ–¹æ³•å’ŒçœŸå€¼ç±»å‹åˆ†ç»„åˆ†æ
        for gt_type in ["train", "test"]:
            print(f"\n--- çœŸå€¼ç±»å‹: {gt_type} ---")
            
            gt_data = df[df['ground_truth_type'] == gt_type]
            
            if gt_data.empty:
                print(f"  æ²¡æœ‰ {gt_type} çš„ç»“æœ")
                continue
            
            # æŒ‰æ–¹æ³•åˆ†ç»„
            methods = gt_data['method_name'].unique()
            
            for method in sorted(methods):
                method_data = gt_data[gt_data['method_name'] == method]
                
                if method_data.empty:
                    continue
                
                print(f"\n  æ–¹æ³•: {method}")
                
                # æ‰¾å‡ºæ¯ä¸ªæŒ‡æ ‡çš„æœ€ä½³ç»“æœ
                best_psnr = method_data.loc[method_data['psnr'].idxmax()]
                best_ssim = method_data.loc[method_data['ssim'].idxmax()]
                best_lpips = method_data.loc[method_data['lpips'].idxmin()]  # LPIPSè¶Šå°è¶Šå¥½
                
                print(f"    æœ€ä½³PSNR: {best_psnr['psnr']:.3f} ({best_psnr['reconstruction_name']})")
                print(f"    æœ€ä½³SSIM: {best_ssim['ssim']:.3f} ({best_ssim['reconstruction_name']})")
                print(f"    æœ€ä½³LPIPS: {best_lpips['lpips']:.3f} ({best_lpips['reconstruction_name']})")
        
        # å…¨å±€æœ€ä½³ç»“æœ
        print(f"\nğŸ¯ å…¨å±€æœ€ä½³ç»“æœ:")
        
        overall_best_psnr = df.loc[df['psnr'].idxmax()]
        overall_best_ssim = df.loc[df['ssim'].idxmax()]
        overall_best_lpips = df.loc[df['lpips'].idxmin()]
        
        print(f"  æœ€ä½³PSNR: {overall_best_psnr['psnr']:.3f}")
        print(f"    - {overall_best_psnr['reconstruction_name']} + {overall_best_psnr['method_name']} vs {overall_best_psnr['ground_truth_type']}")
        
        print(f"  æœ€ä½³SSIM: {overall_best_ssim['ssim']:.3f}")
        print(f"    - {overall_best_ssim['reconstruction_name']} + {overall_best_ssim['method_name']} vs {overall_best_ssim['ground_truth_type']}")
        
        print(f"  æœ€ä½³LPIPS: {overall_best_lpips['lpips']:.3f}")
        print(f"    - {overall_best_lpips['reconstruction_name']} + {overall_best_lpips['method_name']} vs {overall_best_lpips['ground_truth_type']}")


def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) < 2:
        print("âŒ é”™è¯¯: è¯·æŒ‡å®šæ•°æ®é›†åç§°")
        print("ä½¿ç”¨æ–¹æ³•: python calculate_reconstruction_metrics.py <dataset_name>")
        print("ç¤ºä¾‹: python calculate_reconstruction_metrics.py lego2")
        sys.exit(1)
    
    dataset_name = sys.argv[1]
    
    print(f"ğŸ”¥ {dataset_name} é‡å»ºå›¾åƒè´¨é‡æŒ‡æ ‡è®¡ç®—")
    print("=" * 60)
    
    # æ£€æŸ¥condaç¯å¢ƒ
    conda_env = os.environ.get('CONDA_DEFAULT_ENV', 'None')
    print(f"å½“å‰condaç¯å¢ƒ: {conda_env}")
    if conda_env != 'Umain2':
        print("âš ï¸  è­¦å‘Š: æœªåœ¨Umain2ç¯å¢ƒä¸­è¿è¡Œ!")
        print("è¯·ä½¿ç”¨: source ~/miniconda3/etc/profile.d/conda.sh && conda activate Umain2")
    
    try:
        # åˆ›å»ºè®¡ç®—å™¨å¹¶è¿è¡Œ
        calculator = ReconstructionMetricsCalculator(dataset_name)
        
        # å¤„ç†æ‰€æœ‰é‡å»ºç»“æœ
        calculator.process_all_reconstructions()
        
        # ä¿å­˜ç»“æœ
        json_file, csv_file = calculator.save_results()
        
        # åˆ†ææœ€ä½³ç»“æœ
        calculator.analyze_best_results()
        
        print(f"\nğŸ¯ æŒ‡æ ‡è®¡ç®—å®Œæˆ!")
        print(f"ğŸ“Š ç»“æœæ–‡ä»¶: {json_file}")
        print(f"ğŸ“Š CSVæ–‡ä»¶: {csv_file}")
        
    except Exception as e:
        print(f"âŒ è„šæœ¬æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()